{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "\n",
    "y = np.array([0,\n",
    "              1,\n",
    "              1,\n",
    "              1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11373435, 0.23616549, 0.28664866])"
      ]
     },
     "execution_count": 860,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1]"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Logitic_Regression:\n",
    "    def __init__(self, eta=0.01, epoch=100):\n",
    "        self.w = np.array([0.0,0.1,0.2])\n",
    "        self.eta = eta\n",
    "        self.epoch = epoch\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X,0,1,axis=1)\n",
    "        for _ in range(self.epoch):\n",
    "            u = X@self.w\n",
    "            y_hat = self.sigmoid(u)\n",
    "            error = y_hat-y\n",
    "            Grad_w = (self.eta/X.shape[0]) * (error@X)\n",
    "            self.w = self.w - Grad_w\n",
    "            \n",
    "    def predict(self, X):\n",
    "        X = np.insert(X,0,1,axis=1)\n",
    "        u = X@self.w\n",
    "        y_hat = self.sigmoid(u)\n",
    "        return [1 if i>=0.5 else 0 for i in y_hat]\n",
    "            \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "model = Logitic_Regression(eta=0.001 ,epoch=50000)\n",
    "model.fit(X,y)\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Logitic_L2(Logitic_Regression):\n",
    "    def __init__(self, eta=0.01, epoch=100, alpha=0.1):\n",
    "        super().__init__(eta=eta, epoch=epoch)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X,0,1,axis=1)\n",
    "        for _ in range(self.epoch):\n",
    "            u = X@self.w\n",
    "            y_hat = self.sigmoid(u)\n",
    "            error = y_hat-y\n",
    "            grad_w = (1 / X.shape[0]) * (error@X)\n",
    "            grad_w[1:] += self.alpha * self.w[1:]\n",
    "            self.w -= self.eta * grad_w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1]"
      ]
     },
     "execution_count": 863,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Logitic_L2(eta=0.001, epoch=50000, alpha=0.01)\n",
    "model.fit(X,y)\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Logitic_L1(Logitic_Regression):\n",
    "    def __init__(self, eta=0.01, epoch=100, alpha=0.1):\n",
    "        super().__init__(eta=eta, epoch=epoch)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X,0,1,axis=1)\n",
    "        for _ in range(self.epoch):\n",
    "            u = X@self.w\n",
    "            y_hat = self.sigmoid(u)\n",
    "            error = y_hat-y\n",
    "            grad_w = (1 / X.shape[0]) * (error@X)\n",
    "            grad_w[1:] += self.alpha*np.sign(self.w[1:])\n",
    "            self.w -= self.eta * grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1]"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Logitic_L1(epoch=50000, alpha=0.01)\n",
    "model.fit(X,y)\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logitic_L1_L2(Logitic_Regression):\n",
    "    def __init__(self, eta=0.01, epoch=100, alpha=0.1, ratio =0.5):\n",
    "        super().__init__(eta=eta, epoch=epoch)\n",
    "        self.alpha = alpha\n",
    "        self.ratio = ratio\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X,0,1,axis=1)\n",
    "        for _ in range(self.epoch):\n",
    "            u = X@self.w\n",
    "            y_hat = self.sigmoid(u)\n",
    "            error = y_hat-y\n",
    "            grad_w = (1 / X.shape[0]) * (error@X)\n",
    "            grad_w[1:] += self.alpha*((1-self.ratio)*self.w[1:] + self.ratio*(np.sign(self.w[1:])))\n",
    "            self.w -= self.eta * grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1]"
      ]
     },
     "execution_count": 867,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Logitic_L1_L2(epoch=50000, alpha=0.01)\n",
    "model.fit(X,y)\n",
    "model.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
